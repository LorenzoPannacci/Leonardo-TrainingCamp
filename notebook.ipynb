{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":83746,"databundleVersionId":9512704,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":21420.7665,"end_time":"2024-09-05T03:58:23.601125","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-04T22:01:22.834625","version":"2.6.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# import libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AutoTokenizer, AutoModel\nfrom torchvision import transforms, models\nimport pandas as pd\nimport os\nimport numpy as np\nimport logging","metadata":{"papermill":{"duration":6.882883,"end_time":"2024-09-04T22:07:40.499393","exception":false,"start_time":"2024-09-04T22:07:33.616510","status":"completed"},"tags":[],"id":"0e0346ac","execution":{"iopub.status.busy":"2024-09-06T07:20:25.117909Z","iopub.execute_input":"2024-09-06T07:20:25.118182Z","iopub.status.idle":"2024-09-06T07:20:32.970958Z","shell.execute_reply.started":"2024-09-06T07:20:25.118141Z","shell.execute_reply":"2024-09-06T07:20:32.970079Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class MultimodalDataset(Dataset):\n    def __init__(self, dataframe, image_folder, tokenizer, transform):\n        self.dataframe = dataframe\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # get image and text data\n        image_name = self.dataframe.iloc[idx]['image_name']\n        text = self.dataframe.iloc[idx]['description']\n        label = self.dataframe.iloc[idx]['target']\n\n        # load and transform the image\n        img_path = f\"{self.image_folder}/{image_name}\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n\n        # tokenize the text using RoBERTa tokenizer\n        tokens = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n        input_ids = tokens['input_ids'].squeeze(0)  # remove the batch dimension\n        attention_mask = tokens['attention_mask'].squeeze(0)  # remove the batch dimension\n\n        category = self.dataframe.iloc[idx]['object']\n\n        return image, input_ids, attention_mask, category, torch.tensor(label)\n","metadata":{"papermill":{"duration":0.698609,"end_time":"2024-09-04T22:07:41.862042","exception":false,"start_time":"2024-09-04T22:07:41.163433","status":"completed"},"tags":[],"id":"4566c11c","execution":{"iopub.status.busy":"2024-09-06T07:20:33.037425Z","iopub.execute_input":"2024-09-06T07:20:33.037773Z","iopub.status.idle":"2024-09-06T07:20:33.048018Z","shell.execute_reply.started":"2024-09-06T07:20:33.037738Z","shell.execute_reply":"2024-09-06T07:20:33.047245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# image transformation (input size of 224x224)\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"papermill":{"duration":0.672667,"end_time":"2024-09-04T22:07:43.252442","exception":false,"start_time":"2024-09-04T22:07:42.579775","status":"completed"},"tags":[],"id":"7dc1acb4","execution":{"iopub.status.busy":"2024-09-06T07:20:33.049889Z","iopub.execute_input":"2024-09-06T07:20:33.050196Z","iopub.status.idle":"2024-09-06T07:20:33.058591Z","shell.execute_reply.started":"2024-09-06T07:20:33.050163Z","shell.execute_reply":"2024-09-06T07:20:33.057614Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the RoBERTa tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')","metadata":{"papermill":{"duration":3.599875,"end_time":"2024-09-04T22:07:47.514688","exception":false,"start_time":"2024-09-04T22:07:43.914813","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5cd4287d","outputId":"2845041e-b1fe-4bcf-a145-603544d54b90","execution":{"iopub.status.busy":"2024-09-06T07:20:33.059914Z","iopub.execute_input":"2024-09-06T07:20:33.060275Z","iopub.status.idle":"2024-09-06T07:20:34.109626Z","shell.execute_reply.started":"2024-09-06T07:20:33.060233Z","shell.execute_reply":"2024-09-06T07:20:34.108456Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2532dc311f45a6974a375cd5b48bb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0cfdb7295844b586df40b2a80019dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe35b57e3604283b41e61bf2b35fb2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea63e1a07106445da95c03c090bbcbe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8d8de04f0649d085cc4be5450f79e4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the training dataframe\ntrain_df = pd.read_csv('/kaggle/input/multimodal-classification-tc-sep-2024/dataset/train.csv')\n\n# create the custom dataset\ntrain_dataset = MultimodalDataset(train_df,\n                                  image_folder='/kaggle/input/multimodal-classification-tc-sep-2024/dataset/images/train',\n                                  tokenizer=tokenizer,\n                                  transform=image_transform)\n\n# define the DataLoader for training\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)","metadata":{"papermill":{"duration":0.694106,"end_time":"2024-09-04T22:07:48.875767","exception":false,"start_time":"2024-09-04T22:07:48.181661","status":"completed"},"tags":[],"id":"22d72312","execution":{"iopub.status.busy":"2024-09-06T07:20:34.111455Z","iopub.execute_input":"2024-09-06T07:20:34.111843Z","iopub.status.idle":"2024-09-06T07:20:34.171514Z","shell.execute_reply.started":"2024-09-06T07:20:34.111800Z","shell.execute_reply":"2024-09-06T07:20:34.170388Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# image processing network\n\nclass ImageEncoder(nn.Module):\n    def __init__(self):\n        super(ImageEncoder, self).__init__()\n        # load pre-trained model and remove the last fully connected layer\n        self.model = models.convnext_large(pretrained=True)\n        self.model = nn.Sequential(*list(self.model.children())[:-1])  # remove the final classification layer\n\n    def forward(self, images):\n        img_features = self.model(images)  # extract features\n        img_features = img_features.view(img_features.size(0), -1)  # flatten the features\n        return img_features","metadata":{"papermill":{"duration":0.674924,"end_time":"2024-09-04T22:07:50.268334","exception":false,"start_time":"2024-09-04T22:07:49.593410","status":"completed"},"tags":[],"id":"ec323b9b","execution":{"iopub.status.busy":"2024-09-06T07:20:34.173324Z","iopub.execute_input":"2024-09-06T07:20:34.174113Z","iopub.status.idle":"2024-09-06T07:20:34.181419Z","shell.execute_reply.started":"2024-09-06T07:20:34.174064Z","shell.execute_reply":"2024-09-06T07:20:34.180510Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# text encoder network\n\nclass TextEncoder(nn.Module):\n    def __init__(self):\n        super(TextEncoder, self).__init__()\n        self.model = RobertaModel.from_pretrained('roberta-large')\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = outputs.pooler_output\n        return text_features","metadata":{"papermill":{"duration":0.782219,"end_time":"2024-09-04T22:07:51.742723","exception":false,"start_time":"2024-09-04T22:07:50.960504","status":"completed"},"tags":[],"id":"d2cd7497","execution":{"iopub.status.busy":"2024-09-06T07:20:34.185130Z","iopub.execute_input":"2024-09-06T07:20:34.185419Z","iopub.status.idle":"2024-09-06T07:20:34.196455Z","shell.execute_reply.started":"2024-09-06T07:20:34.185387Z","shell.execute_reply":"2024-09-06T07:20:34.195447Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"object_classes = {\"cat\": 0,\n                  \"car\": 1,\n                  \"person\": 2,\n                  \"house\": 3,\n                  \"bus\": 4}\n\ndef create_one_hot_tensor(categories):\n    output = torch.Tensor([])\n    for elem in categories:\n        current = torch.from_numpy(np.array([1 if i == object_classes[elem] else 0 for i in range(0, 5)]))\n        output = torch.cat((output, current), dim=0)\n    return torch.reshape(output, (len(categories), 5))","metadata":{"id":"YMxWI8zFJMCH","execution":{"iopub.status.busy":"2024-09-06T07:20:34.210710Z","iopub.execute_input":"2024-09-06T07:20:34.211014Z","iopub.status.idle":"2024-09-06T07:20:34.218726Z","shell.execute_reply.started":"2024-09-06T07:20:34.210983Z","shell.execute_reply":"2024-09-06T07:20:34.217719Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MultimodalClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(MultimodalClassifier, self).__init__()\n        # image encoder\n        self.image_encoder = ImageEncoder()\n        self.image_encoder.to(\"cuda:1\")\n        # text encoder\n        self.text_encoder = TextEncoder()\n        self.text_encoder.to(\"cuda:0\")\n        # fully connected layers to combine image and text features\n        self.fc = nn.Sequential(\n            nn.Linear(2560, 512)\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        self.fc.to(\"cuda:0\")\n\n    def forward(self, images, input_ids, attention_mask, category):\n        # extract features from images and text\n        img_features = self.image_encoder(images.to(\"cuda:1\"))\n        text_features = self.text_encoder(input_ids.to(\"cuda:0\"), attention_mask.to(\"cuda:0\"))\n        # concatenate the features\n        combined_features = torch.cat((img_features.to(\"cuda:0\"), text_features.to(\"cuda:0\")), dim=1)\n        # pass through the fully connected layers\n        output = self.fc(combined_features.to(\"cuda:0\"))\n        return output\n","metadata":{"papermill":{"duration":0.67594,"end_time":"2024-09-04T22:07:53.094442","exception":false,"start_time":"2024-09-04T22:07:52.418502","status":"completed"},"tags":[],"id":"d14ffa25","execution":{"iopub.status.busy":"2024-09-06T07:20:34.219982Z","iopub.execute_input":"2024-09-06T07:20:34.220293Z","iopub.status.idle":"2024-09-06T07:20:34.232292Z","shell.execute_reply.started":"2024-09-06T07:20:34.220257Z","shell.execute_reply":"2024-09-06T07:20:34.231385Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# remove logging errors from transformers\n\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\nfor logger in loggers:\n    if \"transformers\" in logger.name.lower():\n        logger.setLevel(logging.ERROR)","metadata":{"id":"ET8966mD47Rw","execution":{"iopub.status.busy":"2024-09-06T07:20:34.233628Z","iopub.execute_input":"2024-09-06T07:20:34.234353Z","iopub.status.idle":"2024-09-06T07:20:34.252085Z","shell.execute_reply.started":"2024-09-06T07:20:34.234305Z","shell.execute_reply":"2024-09-06T07:20:34.251206Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# instantiate the model, loss function, and optimizer\nmodel = MultimodalClassifier(num_classes=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n\n# training loop\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for images, input_ids, attention_mask, category, labels in train_loader:\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(images.to(\"cuda:1\"), input_ids.to(\"cuda:0\"), attention_mask.to(\"cuda:0\"), category)\n        loss = criterion(outputs, labels.cuda())\n\n        # backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    # print the average loss for this epoch\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")","metadata":{"papermill":{"duration":20847.19944,"end_time":"2024-09-05T03:55:21.009011","exception":false,"start_time":"2024-09-04T22:07:53.809571","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":406},"id":"4fc1b76f","outputId":"1feea294-124b-4cea-e51f-1132b101c2e0","execution":{"iopub.status.busy":"2024-09-06T07:20:34.253102Z","iopub.execute_input":"2024-09-06T07:20:34.253379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Large_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/convnext_large-ea097f82.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large-ea097f82.pth\n100%|██████████| 755M/755M [00:04<00:00, 186MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084915fd65c3450194c9e04844658ec0"}},"metadata":{}}]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, dataframe, image_folder, tokenizer, transform):\n        self.dataframe = dataframe\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        image_name = self.dataframe.iloc[idx]['image_name']\n        text = self.dataframe.iloc[idx]['description']\n\n        # load and transform the image\n        img_path = f\"{self.image_folder}/{image_name}\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n\n        # tokenize the text\n        tokens = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n        input_ids = tokens['input_ids'].squeeze(0)\n        attention_mask = tokens['attention_mask'].squeeze(0)\n\n        category = self.dataframe.iloc[idx]['object']\n\n        return image, input_ids, attention_mask, category, image_name  # Return image_name for the submission file\n","metadata":{"papermill":{"duration":0.721912,"end_time":"2024-09-05T03:55:23.765276","exception":false,"start_time":"2024-09-05T03:55:23.043364","status":"completed"},"tags":[],"id":"fe2962ef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the test dataframe\ntest_df = pd.read_csv('/kaggle/input/multimodal-classification-tc-sep-2024/dataset/test.csv')\n\n# create the test dataset\ntest_dataset = TestDataset(test_df, image_folder='/kaggle/input/multimodal-classification-tc-sep-2024/dataset/images/test',\n                           tokenizer=tokenizer, transform=image_transform)\n\n# create the DataLoader for the test dataset\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"papermill":{"duration":0.683526,"end_time":"2024-09-05T03:55:25.111921","exception":false,"start_time":"2024-09-05T03:55:24.428395","status":"completed"},"tags":[],"id":"0ee813f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\npredictions = []\n\nwith torch.no_grad():\n    for images, input_ids, attention_mask, category, image_names in test_loader:\n        # forward pass to get predictions\n        outputs = model(images.cuda(), input_ids.cuda(), attention_mask.cuda(), category)\n\n        # get the predicted class (use .argmax to get the class with the highest probability)\n        _, predicted_labels = torch.max(outputs, 1)\n\n        # store the image_name and the corresponding predicted label\n        for image_name, predicted_label in zip(image_names, predicted_labels):\n            predictions.append({'image_name': image_name, 'target': predicted_label.item()})","metadata":{"papermill":{"duration":172.227949,"end_time":"2024-09-05T03:58:18.043631","exception":false,"start_time":"2024-09-05T03:55:25.815682","status":"completed"},"tags":[],"id":"2f5e1fff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the submission file\nsubmission_df = pd.DataFrame(predictions)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.750266,"end_time":"2024-09-05T03:58:19.471612","exception":false,"start_time":"2024-09-05T03:58:18.721346","status":"completed"},"tags":[],"id":"79adbeaf","trusted":true},"execution_count":null,"outputs":[]}]}